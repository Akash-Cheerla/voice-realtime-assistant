<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Voice Assistant</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 20px;
      background: #f4f4f4;
    }
    #container {
      max-width: 700px;
      margin: auto;
      background: white;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 0 10px rgba(0,0,0,0.1);
    }
    h1 {
      text-align: center;
    }
    #log {
      margin-top: 20px;
      padding: 10px;
      background: #f9f9f9;
      border-radius: 8px;
      max-height: 400px;
      overflow-y: auto;
    }
    .user {
      text-align: right;
      background: #dcf8c6;
      margin: 5px;
      padding: 10px;
      border-radius: 10px;
    }
    .assistant {
      text-align: left;
      background: #e6ecf0;
      margin: 5px;
      padding: 10px;
      border-radius: 10px;
    }
    #statusLabel {
      font-weight: bold;
      text-align: center;
      margin-top: 10px;
      font-size: 16px;
    }
    .pulse {
      animation: pulse 1s infinite;
      color: #00b894;
    }
    @keyframes pulse {
      0% { opacity: 1; }
      50% { opacity: 0.5; }
      100% { opacity: 1; }
    }
  </style>
</head>
<body>
  <div id="container">
    <h1>üéß FRI Merchant Application Voice Assistant</h1>
    <div id="log">Initializing assistant...</div>
    <div id="statusLabel" class="pulse">‚è≥ Awaiting microphone...</div>
  </div>

  <script>
    let ws;
    let audioContext, processor, input;
    let bufferSize = 2048;
    let silenceTimer;
    let audioPlayer = new Audio();

    function Float32ArrayToInt16(buffer) {
      const int16 = new Int16Array(buffer.length);
      for (let i = 0; i < buffer.length; i++) {
        let s = Math.max(-1, Math.min(1, buffer[i]));
        int16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
      }
      return int16;
    }

    async function requestMicAndStart() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });

        audioContext = new AudioContext();
        input = audioContext.createMediaStreamSource(stream);
        processor = audioContext.createScriptProcessor(bufferSize, 1, 1);

        connectWebSocket();

        processor.onaudioprocess = e => {
          const raw = e.inputBuffer.getChannelData(0);
          const int16 = Float32ArrayToInt16(raw);
          const b64 = btoa(String.fromCharCode.apply(null, new Uint8Array(new Int16Array(int16).buffer)));

          if (ws && ws.readyState === WebSocket.OPEN) {
            ws.send(JSON.stringify({ type: "audio_chunk", data: b64 }));
          }

          const rms = Math.sqrt(raw.reduce((sum, val) => sum + val * val, 0) / raw.length);
          if (rms > 0.01) {
            clearTimeout(silenceTimer);
            silenceTimer = setTimeout(() => {
              if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ type: "end_stream" }));
                console.log("‚úÖ end_stream sent");
                updateStatus("‚è≥ Processing...", true);
              }
            }, 1400);
          }
        };

        input.connect(processor);
        processor.connect(audioContext.destination);

        updateStatus("üéß Listening...", true);
        logMsg("assistant", "üé§ Microphone access granted. Streaming audio...");
      } catch (err) {
        updateStatus("‚ùå Microphone permission denied", false);
        logMsg("assistant", "‚ùå Please enable your mic to continue.");
      }
    }

    function connectWebSocket() {
      ws = new WebSocket("wss://" + window.location.host + "/ws/audio");

      ws.onopen = () => console.log("‚úÖ WebSocket connected");

      ws.onmessage = e => {
        const msg = JSON.parse(e.data);

        if (msg.type === "transcript") {
          logMsg("user", "üë§ " + msg.text);
          updateStatus("üß† Thinking...", true);
        }
        else if (msg.type === "assistant_reply") {
          logMsg("assistant", "üßê " + msg.text);
          if (msg.audio_b64) {
            updateStatus("üîä Speaking...", true);
            audioPlayer.pause();
            audioPlayer.currentTime = 0;
            audioPlayer.src = "data:audio/wav;base64," + msg.audio_b64;
            audioPlayer.play();
            audioPlayer.onended = () => {
              updateStatus("üéß Listening...");
              audioPlayer.src = "";
            };
          } else {
            updateStatus("üéß Listening...", true);
          }
        }
        else if (msg.type === "interrupt_audio") {
          audioPlayer.pause();
          audioPlayer.currentTime = 0;
          audioPlayer.src = "";
        }
        else if (msg.type === "error") {
          logMsg("assistant", "‚ùå Error: " + msg.message);
          updateStatus("‚ö†Ô∏è Something went wrong", false);
        }
      };
    }

    function updateStatus(text, pulse) {
      const label = document.getElementById("statusLabel");
      label.textContent = text;
      label.className = pulse ? "pulse" : "";
    }

    function logMsg(role, text) {
      const div = document.createElement("div");
      div.className = role;
      div.textContent = text;
      document.getElementById("log").appendChild(div);
      document.getElementById("log").scrollTop = document.getElementById("log").scrollHeight;
    }

    window.onload = () => {
      const btn = document.createElement("button");
      btn.textContent = "‚ñ∂Ô∏è Start Assistant";
      btn.style.marginTop = "10px";
      btn.onclick = () => {
        btn.remove();
        requestMicAndStart();
      };
      document.getElementById("container").appendChild(btn);
    };

    window.onbeforeunload = () => {
      if (ws) ws.close();
    };
  </script>
</body>
</html>
