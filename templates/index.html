<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Voice Assistant</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 20px;
      background: #f4f4f4;
    }
    #container {
      max-width: 700px;
      margin: auto;
      background: white;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 0 10px rgba(0,0,0,0.1);
    }
    h1 {
      text-align: center;
    }
    #log {
      margin-top: 20px;
      padding: 10px;
      background: #f9f9f9;
      border-radius: 8px;
      max-height: 400px;
      overflow-y: auto;
    }
    .user {
      text-align: right;
      background: #dcf8c6;
      margin: 5px;
      padding: 10px;
      border-radius: 10px;
    }
    .assistant {
      text-align: left;
      background: #e6ecf0;
      margin: 5px;
      padding: 10px;
      border-radius: 10px;
    }
  </style>
</head>
<body>
  <div id="container">
    <h1>üéôÔ∏è FRI Merchant Application Voice Assistant</h1>
    <div id="log">Initializing assistant...</div>
  </div>

  <script>
    let ws, audioContext, processor, input;
    let bufferSize = 2048;
    let lastAudioTime = Date.now();
    const silenceThreshold = 500;

    function Float32ArrayToInt16(buffer) {
      let l = buffer.length;
      let buf = new Int16Array(l);
      while (l--) buf[l] = Math.min(1, buffer[l]) * 0x7FFF;
      return buf;
    }

    function connectWebSocket() {
      ws = new WebSocket("wss://" + window.location.host + "/ws/audio");

      ws.onmessage = e => {
        const msg = JSON.parse(e.data);
        if (msg.type === "transcript") {
          document.getElementById("log").innerHTML += `<div class='user'>üë§ ${msg.text}</div>`;
        } else if (msg.type === "assistant_reply") {
          document.getElementById("log").innerHTML += `<div class='assistant'>ü§ñ ${msg.text}</div>`;
          const audio = new Audio("data:audio/wav;base64," + msg.audio_b64);
          audio.play();
        }
      };
    }

    async function requestMicAndStart() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        audioContext = new AudioContext();
        input = audioContext.createMediaStreamSource(stream);
        processor = audioContext.createScriptProcessor(bufferSize, 1, 1);

        processor.onaudioprocess = e => {
          const now = Date.now();
          const data = e.inputBuffer.getChannelData(0);
          const int16 = Float32ArrayToInt16(data);
          const hasSpeech = data.some(sample => Math.abs(sample) > 0.01);

          if (hasSpeech) {
            lastAudioTime = now;
            if (ws && ws.readyState === WebSocket.OPEN) {
              ws.send(JSON.stringify({ type: "audio_chunk", data: btoa(String.fromCharCode(...int16)) }));
            }
          }

          if (now - lastAudioTime > silenceThreshold) {
            if (ws && ws.readyState === WebSocket.OPEN) {
              ws.send(JSON.stringify({ type: "end_stream" }));
            }
            lastAudioTime = Infinity;
          }
        };

        input.connect(processor);
        processor.connect(audioContext.destination);

        connectWebSocket();
        document.getElementById("log").innerHTML += `<div class='assistant'>üé§ Microphone access granted. Streaming audio...</div>`;
      } catch {
        document.getElementById("log").innerHTML += `<div class='assistant'>‚ùå Microphone access denied. Please enable it to continue.</div>`;
      }
    }

    window.onload = () => {
      const startBtn = document.createElement("button");
      startBtn.textContent = "‚ñ∂Ô∏è Start Assistant (Grants Microphone access)";
      startBtn.style.marginTop = "10px";
      startBtn.onclick = () => {
        startBtn.remove();
        requestMicAndStart();
      };
      document.getElementById("container").appendChild(startBtn);
    };
  </script>
</body>
</html>
